{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    " \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import requests \n",
    "import torch \n",
    "import torch.onnx \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module): \n",
    "    def __init__(self, upscale_factor): \n",
    "        super().__init__() \n",
    "        self.upscale_factor = upscale_factor \n",
    "        self.img_upsampler = nn.Upsample( \n",
    "            scale_factor=self.upscale_factor, \n",
    "            mode='bicubic', \n",
    "            align_corners=False) \n",
    " \n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=9,padding=4) \n",
    "        self.conv2 = nn.Conv2d(64,32,kernel_size=1,padding=0) \n",
    "        self.conv3 = nn.Conv2d(32,3,kernel_size=5,padding=2) \n",
    " \n",
    "        self.relu = nn.ReLU() \n",
    " \n",
    "    def forward(self, x): \n",
    "        x = self.img_upsampler(x) \n",
    "        out = self.relu(self.conv1(x)) \n",
    "        out = self.relu(self.conv2(out)) \n",
    "        out = self.conv3(out) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_torch_model(): \n",
    "    torch_model = SuperResolutionNet(upscale_factor=3) \n",
    " \n",
    "    state_dict = torch.load('srcnn.pth')['state_dict'] \n",
    " \n",
    "    # Adapt the checkpoint \n",
    "    for old_key in list(state_dict.keys()): \n",
    "        new_key = '.'.join(old_key.split('.')[1:]) \n",
    "        state_dict[new_key] = state_dict.pop(old_key) \n",
    " \n",
    "    torch_model.load_state_dict(state_dict) \n",
    "    torch_model.eval() \n",
    "    return torch_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(768, 768, 3)\n"
     ]
    }
   ],
   "source": [
    "# Download checkpoint and test image \n",
    "urls = ['https://download.openmmlab.com/mmediting/restorers/srcnn/srcnn_x4k915_1x16_1000k_div2k_20200608-4186f232.pth', \n",
    "    'https://raw.githubusercontent.com/open-mmlab/mmediting/master/tests/data/face/000001.png'] \n",
    "names = ['srcnn.pth', 'face.png'] \n",
    "for url, name in zip(urls, names): \n",
    "    if not os.path.exists(name): \n",
    "        open(name, 'wb').write(requests.get(url).content) \n",
    "\n",
    "model = init_torch_model() \n",
    "input_img = cv2.imread('face.png').astype(np.float32) \n",
    "print(input_img.shape)\n",
    " \n",
    "# HWC to NCHW \n",
    "input_img = np.transpose(input_img, [2, 0, 1]) \n",
    "input_img = np.expand_dims(input_img, 0) \n",
    " \n",
    "# Inference \n",
    "torch_output = model(torch.from_numpy(input_img)).detach().numpy() \n",
    " \n",
    "# NCHW to HWC \n",
    "torch_output = np.squeeze(torch_output, 0) \n",
    "torch_output = np.clip(torch_output, 0, 255) \n",
    "torch_output = np.transpose(torch_output, [1, 2, 0]).astype(np.uint8) \n",
    " \n",
    "# Show image \n",
    "cv2.imwrite(\"face_torch.png\", torch_output)\n",
    "print(torch_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 3, 256, 256, strides=[196608, 65536, 256, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(64, 3, 9, 9, strides=[243, 81, 9, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(32, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv3.weight : Float(3, 32, 5, 5, strides=[800, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv3.bias : Float(3, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/img_upsampler/Constant_output_0 : Float(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 1  1  3  3 [ CPUFloatType{4} ], onnx_name=\"/img_upsampler/Constant\"](), scope: __main__.SuperResolutionNet::/torch.nn.modules.upsampling.Upsample::img_upsampler # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:4073:0\n",
      "  %onnx::Resize_10 : Tensor? = prim::Constant(), scope: __main__.SuperResolutionNet::/torch.nn.modules.upsampling.Upsample::img_upsampler # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:4073:0\n",
      "  %/img_upsampler/Resize_output_0 : Float(1, 3, 768, 768, strides=[1769472, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"half_pixel\", cubic_coeff_a=-0.75, mode=\"cubic\", nearest_mode=\"floor\", onnx_name=\"/img_upsampler/Resize\"](%input, %onnx::Resize_10, %/img_upsampler/Constant_output_0), scope: __main__.SuperResolutionNet::/torch.nn.modules.upsampling.Upsample::img_upsampler # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:4073:0\n",
      "  %/conv1/Conv_output_0 : Float(1, 64, 768, 768, strides=[37748736, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[9, 9], pads=[4, 4, 4, 4], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%/img_upsampler/Resize_output_0, %conv1.weight, %conv1.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv1 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/relu/Relu_output_0 : Float(1, 64, 768, 768, strides=[37748736, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu/Relu\"](%/conv1/Conv_output_0), scope: __main__.SuperResolutionNet::/torch.nn.modules.activation.ReLU::relu # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:1500:0\n",
      "  %/conv2/Conv_output_0 : Float(1, 32, 768, 768, strides=[18874368, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/relu/Relu_output_0, %conv2.weight, %conv2.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv2 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/relu_1/Relu_output_0 : Float(1, 32, 768, 768, strides=[18874368, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu_1/Relu\"](%/conv2/Conv_output_0), scope: __main__.SuperResolutionNet::/torch.nn.modules.activation.ReLU::relu # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:1500:0\n",
      "  %output : Float(1, 3, 768, 768, strides=[1769472, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1], onnx_name=\"/conv3/Conv\"](%/relu_1/Relu_output_0, %conv3.weight, %conv3.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv3 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  return (%output)\n",
      "\n",
      "Exported model to ONNX\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model, x, \"srcnn.onnx\", verbose=True, \n",
    "                      input_names=['input'], output_names=['output']) \n",
    "    print(\"Exported model to ONNX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"srcnn.onnx\")\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"Model is valid\")\n",
    "except onnx.onnx_cpp2py_export.checker.ValidationError as e:\n",
    "    print(\"Model is invalid: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\"srcnn.onnx\")\n",
    "ort_input = {\"input\": input_img}\n",
    "ort_output = ort_session.run([\"output\"], ort_input)[0]\n",
    "\n",
    "ort_output = np.squeeze(ort_output, 0)\n",
    "ort_output = np.clip(ort_output, 0, 255)\n",
    "ort_output = np.transpose(ort_output, [1, 2, 0]).astype(np.uint8)\n",
    "cv2.imwrite(\"face_ort.png\", ort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
