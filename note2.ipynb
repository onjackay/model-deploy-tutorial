{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import requests \n",
    "import torch \n",
    "import torch.onnx \n",
    "from torch import nn \n",
    "from torch.nn.functional import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInterpolate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, scales):\n",
    "        return interpolate(input,\n",
    "                           scale_factor=scales[-2:].tolist(),\n",
    "                           mode='bicubic',\n",
    "                           align_corners=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, input, scales):\n",
    "        return g.op('Resize', \n",
    "                    input, \n",
    "                    g.op('Constant', value_t=torch.tensor([], dtype=torch.float32)),\n",
    "                    scales, \n",
    "                    mode_s='cubic',\n",
    "                    coordinate_transformation_mode_s='half_pixel',\n",
    "                    cubic_coeff_a_f=-0.75,\n",
    "                    nearest_mode_s='floor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    " \n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=9,padding=4) \n",
    "        self.conv2 = nn.Conv2d(64,32,kernel_size=1,padding=0) \n",
    "        self.conv3 = nn.Conv2d(32,3,kernel_size=5,padding=2) \n",
    " \n",
    "        self.relu = nn.ReLU() \n",
    " \n",
    "    def forward(self, x, upsample_factor): \n",
    "        x = MyInterpolate.apply(x, upsample_factor)\n",
    "        out = self.relu(self.conv1(x)) \n",
    "        out = self.relu(self.conv2(out)) \n",
    "        out = self.conv3(out) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_torch_model(): \n",
    "    torch_model = SuperResolutionNet() \n",
    " \n",
    "    state_dict = torch.load('srcnn.pth')['state_dict'] \n",
    " \n",
    "    # Adapt the checkpoint \n",
    "    for old_key in list(state_dict.keys()): \n",
    "        new_key = '.'.join(old_key.split('.')[1:]) \n",
    "        state_dict[new_key] = state_dict.pop(old_key) \n",
    " \n",
    "    torch_model.load_state_dict(state_dict) \n",
    "    torch_model.eval() \n",
    "    return torch_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(768, 768, 3)\n"
     ]
    }
   ],
   "source": [
    "model = init_torch_model() \n",
    "input_img = cv2.imread('face.png').astype(np.float32) \n",
    "print(input_img.shape)\n",
    " \n",
    "# HWC to NCHW \n",
    "input_img = np.transpose(input_img, [2, 0, 1]) \n",
    "input_img = np.expand_dims(input_img, 0) \n",
    " \n",
    "# Inference \n",
    "factor = torch.tensor([1, 1, 3, 3])\n",
    "torch_output = model(torch.from_numpy(input_img), \n",
    "                     factor).detach().numpy() \n",
    " \n",
    "# NCHW to HWC \n",
    "torch_output = np.squeeze(torch_output, 0) \n",
    "torch_output = np.clip(torch_output, 0, 255) \n",
    "torch_output = np.transpose(torch_output, [1, 2, 0]).astype(np.uint8) \n",
    " \n",
    "# Show image \n",
    "cv2.imwrite(\"face_torch.png\", torch_output)\n",
    "print(torch_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48659/2618028903.py:6: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale_factor=scales[-2:].tolist(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 3, 256, 256, strides=[196608, 65536, 256, 1], requires_grad=0, device=cpu),\n",
      "      %factor : Float(4, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(64, 3, 9, 9, strides=[243, 81, 9, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(32, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv3.weight : Float(3, 32, 5, 5, strides=[800, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv3.bias : Float(3, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/Constant_output_0 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ], onnx_name=\"/Constant\"](), scope: __main__.SuperResolutionNet::\n",
      "  %/Resize_output_0 : Float(*, *, *, *, strides=[1769472, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"half_pixel\", cubic_coeff_a=-0.75, mode=\"cubic\", nearest_mode=\"floor\", onnx_name=\"/Resize\"](%input, %/Constant_output_0, %factor), scope: __main__.SuperResolutionNet:: # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/autograd/function.py:598:0\n",
      "  %/conv1/Conv_output_0 : Float(*, 64, *, *, strides=[37748736, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[9, 9], pads=[4, 4, 4, 4], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%/Resize_output_0, %conv1.weight, %conv1.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv1 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/relu/Relu_output_0 : Float(*, 64, *, *, strides=[37748736, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu/Relu\"](%/conv1/Conv_output_0), scope: __main__.SuperResolutionNet::/torch.nn.modules.activation.ReLU::relu # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:1500:0\n",
      "  %/conv2/Conv_output_0 : Float(*, 32, *, *, strides=[18874368, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/relu/Relu_output_0, %conv2.weight, %conv2.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv2 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/relu_1/Relu_output_0 : Float(*, 32, *, *, strides=[18874368, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu_1/Relu\"](%/conv2/Conv_output_0), scope: __main__.SuperResolutionNet::/torch.nn.modules.activation.ReLU::relu # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/functional.py:1500:0\n",
      "  %output : Float(*, 3, *, *, strides=[1769472, 589824, 768, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1], onnx_name=\"/conv3/Conv\"](%/relu_1/Relu_output_0, %conv3.weight, %conv3.bias), scope: __main__.SuperResolutionNet::/torch.nn.modules.conv.Conv2d::conv3 # /root/projects/modelDep/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  return (%output)\n",
      "\n",
      "Exported model to ONNX\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "factor = torch.tensor([1, 1, 3, 3], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model, (x, factor), \"srcnn2.onnx\", verbose=True, \n",
    "                      input_names=[\"input\", \"factor\"], output_names=[\"output\"]) \n",
    "    print(\"Exported model to ONNX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"srcnn2.onnx\")\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"Model is valid\")\n",
    "except onnx.onnx_cpp2py_export.checker.ValidationError as e:\n",
    "    print(\"Model is invalid: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\"srcnn2.onnx\")\n",
    "input_factor = np.array([1, 1, 4, 4], dtype=np.float32)\n",
    "ort_input = {\"input\": input_img, \"factor\": input_factor}\n",
    "ort_output = ort_session.run([\"output\"], ort_input)[0]\n",
    "\n",
    "ort_output = np.squeeze(ort_output, 0)\n",
    "ort_output = np.clip(ort_output, 0, 255)\n",
    "ort_output = np.transpose(ort_output, [1, 2, 0]).astype(np.uint8)\n",
    "cv2.imwrite(\"face_ort4x4.png\", ort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
